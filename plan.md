# ğŸš€ High-Performance People Search System

A scalable, performant data ingestion and search system built to handle **100+ million rows** of people records from CSV using **ClickHouse** for blazing-fast queries and **PostgreSQL** for authentication, user management, and logging.

---

## ğŸ§  Use Case

We have a large dataset (100M+ records) representing individual profiles with fields like "mobile","name","fname","address","alt","circle","id","email"
- Perform **fast searches** (full or partial matches).
- Export filtered search results to CSV.
- Have **search usage tracking**, limits, and audit logging.
- Allow **admin-controlled user access** (demo or permanent users).
- Monitor daily logins, search frequency, and activity patterns.

---

## ğŸ› ï¸ Tech Stack

| Layer       | Tool                      | Purpose                                  |
|------------|---------------------------|------------------------------------------|
| Backend    | Golang                    | API Server, Services                     |
| Database   | **ClickHouse**            | Primary for search, read-heavy ops       |
| Database   | **PostgreSQL**            | Auth, user mgmt, tracking, audit logs    |
| Queue      | Optional (e.g. Kafka)     | Async CSV ingestion (if needed)          |
| Deployment | Docker, Nginx, systemd    | Service control and reverse proxy        |

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ cmd
â”‚ â””â”€â”€ main.go
â”œâ”€â”€ config
â”‚ â””â”€â”€ config.yaml / .env
â”œâ”€â”€ handlers
â”‚ â””â”€â”€ user.go
â”‚ â””â”€â”€ search.go
â”œâ”€â”€ services
â”‚ â””â”€â”€ auth.go
â”‚ â””â”€â”€ csv_ingest.go
â”œâ”€â”€ models
â”‚ â””â”€â”€ postgres_models.go
â”‚ â””â”€â”€ clickhouse_models.go
â”œâ”€â”€ database
â”‚ â””â”€â”€ clickhouse.go
â”‚ â””â”€â”€ postgres.go
â”œâ”€â”€ utils
â”‚ â””â”€â”€ csv.go
â”‚ â””â”€â”€ logger.go
â”œâ”€â”€ migrations
â”‚ â””â”€â”€ schema.sql
â”œâ”€â”€ README.md

markdown
Copy
Edit

---

## ğŸ§© Key Features

### ğŸ” Search System
- **Fast search** via ClickHouse.
- Support for:
  - `mobile`, `name`, `fatherName`, `address`, `email`, `circle`
  - "Search within search"
  - AND/OR logic toggle
  - Partial + full match
- Search result limit (e.g. 10,000 rows max per query).
- Option to export search result as CSV.
- Admin view to see latest searches and results.

### ğŸ‘¤ User Management
- **Types**:
  - Demo (expire after X days)
  - Permanent
- Admins can:
  - View all users
  - Disable users (prevent login)
  - Set search/export limits
- User activity tracked:
  - Login time
  - Search terms
  - Export count
  - API usage per day

### ğŸ“„ CSV Ingestion
- Upload via admin panel or backend
- Ingests 100M+ rows into ClickHouse using:
  - `clickhouse-client`
  - Golang batch insert
- UUIDs assigned per row
- `master_id` from CSV retained

---

## ğŸ—ƒï¸ ClickHouse Schema (Highly Optimized)

```sql
CREATE TABLE people
(
    id UUID DEFAULT generateUUIDv4(),   -- internal UUID
    master_id String,                   -- mapped from CSV "id"
    mobile String,
    name String,
    fname String,
    address String,
    alt String,
    circle String,
    email String,
    created_at DateTime DEFAULT now()
)
ENGINE = MergeTree()
ORDER BY (mobile, name, master_id);

ğŸ“¥ CSV Field Mapping
CSV Field	ClickHouse Column

mobile	mobile-1
name	name -2
fname	fname-3
address	address-4
alt	alt-5
circle	circle-5
id	master_id-7
email	email-8 
âœ… Notes:

phone is added as an alternate fast filter.

ORDER BY improves range scans and indexing.

Partial search done via LIKE/ILIKE or tokens.

ğŸ˜ PostgreSQL Schema
sql
Copy
Edit
-- Users table
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name TEXT,
    email TEXT UNIQUE,
    password_hash TEXT,
    user_type TEXT CHECK (user_type IN ('DEMO', 'PERMANENT')),
    expires_at TIMESTAMP,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT now(),
    updated_at TIMESTAMP DEFAULT now()
);

-- Login activity
CREATE TABLE logins (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    login_time TIMESTAMP DEFAULT now()
);

-- Search logs
CREATE TABLE searches (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    search_query TEXT,
    search_time TIMESTAMP DEFAULT now(),
    result_count INT
);

-- Export logs
CREATE TABLE exports (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    exported_at TIMESTAMP DEFAULT now(),
    row_count INT
);
ğŸ” Authentication Flow
JWT-based login system.

Only active users with valid email/password and non-expired accounts can log in.

Middleware to validate JWT on all protected routes.

Admin UI can manage users (enable/disable, extend expiry).

ğŸ” Example API Endpoints
Method	Endpoint	Purpose
POST	/login	Login user and return JWT
POST	/search	Perform full-text search
GET	/me	Get current user info
POST	/export	Export last search to CSV
GET	/admin/users	List all users
PATCH	/admin/users/:id	Toggle active/inactive
POST	/admin/ingest	Upload new CSV and ingest

ğŸ§ª Search Logic Example
json
Copy
Edit
POST /search
{
  "query": "delhi",
  "fields": ["address", "circle"],
  "logic": "OR",
  "searchWithin": true
}
â³ Limits and Throttling
Max searches per user/day: e.g. 500

Max CSV export per day: 3

Max rows per search: 10,000

Admins can adjust limits via dashboard

ğŸš€ Performance Tips
Use ClickHouse INSERT INTO people FORMAT CSV for ingestion.

Use batch insert in Golang for ingestion in 1M+ chunks.

Index smartly in PostgreSQL.

Add logs and dashboards with tools like Grafana + Prometheus (optional).

Optionally add Redis caching for repeated search terms.

ğŸ‘¨â€ğŸ’» Developer Setup
bash
Copy
Edit
# ClickHouse
docker run -d --name ch \
  -p 8123:8123 -p 9000:9000 \
  clickhouse/clickhouse-server

# PostgreSQL
docker run -d --name pg \
  -e POSTGRES_PASSWORD=secret \
  -p 5432:5432 postgres

# Golang Server
go run cmd/main.go
ğŸ§  Summary
ğŸ”¥ ClickHouse for ultra-fast analytics & filtering.

ğŸ˜ PostgreSQL for relational data, audit logs, and users.

ğŸ”’ Secure, scalable and audit-friendly.

ğŸ’¼ Suitable for enterprise-grade search systems.
